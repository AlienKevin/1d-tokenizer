#!/usr/bin/env python3
"""
Plot heatmap of token frequency across generation steps and token positions.

This script reads the trace data from NPZ files generated by sample_imagenet_titok.py
with --sample-tokens-only option and creates a heatmap visualization.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Hashable
try:
    from scipy.optimize import linear_sum_assignment
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


def load_traces(npz_path):
    """Load traces and conditions from NPZ file."""
    data = np.load(npz_path)
    traces = data['traces']  # [num_samples, num_steps, seq_len]
    conditions = data.get('conditions', None)  # [num_samples] - may not exist in older files
    print(f"Loaded traces with shape: {traces.shape}")
    if conditions is not None:
        print(f"Loaded conditions with shape: {conditions.shape}")
        print(f"Condition range: {conditions.min()} - {conditions.max()}")
    else:
        print("No conditions found in NPZ file")
    return traces, conditions


def compute_token_frequency_heatmap(traces, condition_mask=None):
    """
    Compute token frequency heatmap across generation steps and token positions.
    
    Args:
        traces: numpy array of shape [num_samples, num_steps, seq_len]
        condition_mask: boolean mask to select specific samples (optional)
        
    Returns:
        heatmap: numpy array of shape [num_steps, seq_len] with token frequencies
    """
    if condition_mask is not None:
        traces = traces[condition_mask]
    
    num_samples, num_steps, seq_len = traces.shape
    
    # Initialize frequency matrix: [num_steps, seq_len]
    # Each cell will contain the count of unique tokens at that (step, position)
    frequency_matrix = np.zeros((num_steps, seq_len))
    
    for step in range(num_steps):
        for pos in range(seq_len):
            # Get all token values at this (step, position) across all samples
            tokens_at_pos = traces[:, step, pos]
            # Count unique tokens (excluding mask tokens if needed)
            unique_tokens = len(np.unique(tokens_at_pos))
            frequency_matrix[step, pos] = unique_tokens
    
    return frequency_matrix


def compute_token_diversity_heatmap(traces, mask_token_id=4096, condition_mask=None):
    """
    Compute token diversity heatmap (entropy-based measure).
    
    Args:
        traces: numpy array of shape [num_samples, num_steps, seq_len]
        mask_token_id: ID of the mask token to exclude from diversity calculation
        condition_mask: boolean mask to select specific samples (optional)
        
    Returns:
        heatmap: numpy array of shape [num_steps, seq_len] with diversity scores
    """
    if condition_mask is not None:
        traces = traces[condition_mask]
    
    num_samples, num_steps, seq_len = traces.shape
    diversity_matrix = np.zeros((num_steps, seq_len))
    
    for step in range(num_steps):
        for pos in range(seq_len):
            tokens_at_pos = traces[:, step, pos]
            
            # Filter out mask tokens for diversity calculation
            non_mask_tokens = tokens_at_pos[tokens_at_pos != mask_token_id]
            
            if len(non_mask_tokens) == 0:
                diversity_matrix[step, pos] = 0
                continue
                
            # Calculate entropy as diversity measure
            unique_tokens, counts = np.unique(non_mask_tokens, return_counts=True)
            probabilities = counts / len(non_mask_tokens)
            entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
            diversity_matrix[step, pos] = entropy
    
    return diversity_matrix


def analyze_token_changes(traces, condition_mask=None):
    """
    Analyze when tokens change during generation (similar to sampling pattern analysis).
    
    Args:
        traces: numpy array of shape [num_samples, num_steps, seq_len]
        condition_mask: boolean mask to select specific samples (optional)
        
    Returns:
        step_position_changes: dict mapping step -> Counter of position change frequencies
    """
    if condition_mask is not None:
        traces = traces[condition_mask]
    
    num_samples, num_steps, seq_len = traces.shape
    step_position_changes = defaultdict(Counter)
    
    # For each sample, track when tokens change at each position
    for sample_idx in range(num_samples):
        sample_trace = traces[sample_idx]  # [num_steps, seq_len]
        
        for step in range(1, num_steps):  # Start from step 1 to compare with previous
            for pos in range(seq_len):
                # Check if token changed from previous step
                if sample_trace[step, pos] != sample_trace[step-1, pos]:
                    step_position_changes[step][pos] += 1
    
    return step_position_changes


def _natural_num(x: Hashable, default: int = 10**9) -> int:
    """Return an int if x looks like an integer (e.g., 'step12' -> 12), else default for stable sort."""
    s = str(x)
    digits = "".join(ch for ch in s if ch.isdigit())
    return int(digits) if digits else default


def optimal_step_position_assignment(
    step_position_counts: Dict[Hashable, Counter],
    positions: List[Hashable] = None,
) -> Tuple[Dict[Hashable, Hashable], int]:
    """
    Find a one-to-one assignment of token positions to steps that maximizes the sum of counts.

    Args:
        step_position_counts: mapping like {step_k: Counter({pos_i: count, ...}), ...}
        positions: optional explicit list of candidate token positions. If None, uses the union
                   of all positions appearing in the counters.

    Returns:
        assignment: {step -> position}
        total: sum of counts for the chosen (step, position) pairs
    """
    if not HAS_SCIPY:
        print("Warning: scipy not available, skipping optimal assignment calculation")
        return {}, 0
        
    # Order steps (try numeric order if keys include numbers like "Step 12")
    steps = sorted(step_position_counts.keys(), key=_natural_num)

    # Build the list of positions (columns)
    if positions is None:
        pos_set = set()
        for c in step_position_counts.values():
            pos_set.update(c.keys())
        positions = sorted(pos_set, key=lambda p: (isinstance(p, str), p))
    else:
        positions = list(positions)

    n_rows, n_cols = len(steps), len(positions)

    # If fewer columns than rows, add dummy positions (all zeros) so each step gets a unique column
    if n_cols < n_rows:
        dummy_needed = n_rows - n_cols
        positions += [f"__DUMMY_{i}__" for i in range(dummy_needed)]
        n_cols = len(positions)

    # Build counts matrix (rows=steps, cols=positions)
    counts = np.zeros((n_rows, n_cols), dtype=np.int64)
    for i, step in enumerate(steps):
        cnt = step_position_counts.get(step, Counter())
        for j, pos in enumerate(positions):
            counts[i, j] = cnt.get(pos, 0)

    # Maximize counts == minimize negative counts
    row_ind, col_ind = linear_sum_assignment(-counts)

    # Construct assignment; drop any dummy columns
    assignment = {}
    total = 0
    for r, c in zip(row_ind, col_ind):
        pos = positions[c]
        if isinstance(pos, str) and pos.startswith("__DUMMY_"):
            continue  # ignore dummy matches
        assignment[steps[r]] = pos
        total += int(counts[r, c])

    return assignment, total


def create_token_change_heatmap(step_position_changes, num_steps, seq_len, output_path="token_changes_heatmap.png"):
    """Create a heatmap showing when tokens change at each position across steps."""
    # Create matrix of step x position change frequencies
    matrix = np.zeros((num_steps, seq_len))
    
    for step_idx in range(num_steps):
        if step_idx in step_position_changes:
            for position, count in step_position_changes[step_idx].items():
                matrix[step_idx, position] = count
    
    plt.figure(figsize=(16, 10))
    
    # Use a custom colormap for better visibility (matching reference)
    sns.heatmap(matrix, 
                annot=False, 
                cmap='YlOrRd', 
                cbar_kws={'label': 'Token Change Frequency'},
                xticklabels=range(seq_len),
                yticklabels=[f'Step {i+1}' for i in range(num_steps)])
    
    plt.title('Token Change Frequency Heatmap\n(When tokens change from previous step)', fontsize=16, fontweight='bold')
    plt.xlabel(f'Token Position (0-{seq_len-1})', fontsize=12)
    plt.ylabel('Generation Step', fontsize=12)
    plt.xticks(rotation=0)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Token change heatmap saved to: {output_path}")
    plt.close()
    
    return matrix


def create_combined_token_change_heatmaps(all_changes, condition_changes, condition_labels, num_steps, seq_len, output_path="token_changes_combined_heatmap.png"):
    """Create combined token change heatmaps: aggregated + 3 condition-specific."""
    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    
    # Convert step_position_changes to matrices
    def changes_to_matrix(step_position_changes):
        matrix = np.zeros((num_steps, seq_len))
        for step_idx in range(num_steps):
            if step_idx in step_position_changes:
                for position, count in step_position_changes[step_idx].items():
                    matrix[step_idx, position] = count
        return matrix
    
    # Create matrices
    all_matrix = changes_to_matrix(all_changes)
    condition_matrices = [changes_to_matrix(changes) for changes in condition_changes]
    
    # Plot aggregated heatmap (use its own scale)
    sns.heatmap(
        all_matrix,
        ax=axes[0],
        cmap='YlOrRd',
        cbar=True,
        cbar_kws={'label': 'All Classes'},
        xticklabels=False,
        yticklabels=True
    )
    axes[0].set_title('Token Changes\n(All Classes)', fontsize=12, fontweight='bold')
    axes[0].set_xlabel('Token Position', fontsize=10)
    axes[0].set_ylabel('Generation Step', fontsize=10)
    
    # Plot condition-specific heatmaps (each with its own scale)
    for i, (matrix, label) in enumerate(zip(condition_matrices, condition_labels)):
        ax = axes[i + 1]
        
        # Use individual normalization for each condition
        sns.heatmap(
            matrix,
            ax=ax,
            cmap='YlOrRd',
            cbar=(i == 2),  # Only show colorbar on the last subplot
            cbar_kws={'label': f'Class {label}'} if i == 2 else None,
            xticklabels=False,
            yticklabels=False
        )
        ax.set_title(f'Token Changes\n(Class {label})', fontsize=12, fontweight='bold')
        ax.set_xlabel('Token Position', fontsize=10)
        
        # Add x-axis ticks for better readability
        tick_positions = [0, seq_len//2, seq_len-1]
        ax.set_xticks(tick_positions)
        ax.set_xticklabels([str(pos) for pos in tick_positions])
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Combined token change heatmap saved to: {output_path}")
    plt.close()


def create_step_distribution_charts(step_position_changes, num_steps, output_path="step_distributions.png"):
    """Create bar charts showing top positions for each step (matching reference implementation)."""
    # Select a subset of steps to display (every 4th step for readability)
    steps_to_show = list(range(0, num_steps, max(1, num_steps // 8)))
    if (num_steps - 1) not in steps_to_show:
        steps_to_show.append(num_steps - 1)
    
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for i, step_idx in enumerate(steps_to_show[:8]):
        if i >= len(axes):
            break
            
        ax = axes[i]
        
        if step_idx in step_position_changes:
            # Get top 10 positions for this step
            top_positions = step_position_changes[step_idx].most_common(10)
            
            if top_positions:
                positions = [pos for pos, count in top_positions]
                counts = [count for pos, count in top_positions]
                
                bars = ax.bar(range(len(positions)), counts, 
                             color=plt.cm.viridis(np.linspace(0, 1, len(positions))))
                ax.set_xticks(range(len(positions)))
                ax.set_xticklabels([f'P{pos}' for pos in positions], rotation=45)
                ax.set_ylabel('Frequency')
                ax.set_title(f'Step {step_idx + 1} - Top Positions')
                
                # Add value labels on bars
                for bar, count in zip(bars, counts):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                           f'{count}', ha='center', va='bottom', fontsize=8)
    
    # Hide unused subplots
    for i in range(len(steps_to_show), len(axes)):
        axes[i].set_visible(False)
    
    plt.suptitle('Token Position Change Frequencies by Generation Step', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Step distribution charts saved to: {output_path}")
    plt.close()


def create_summary_statistics_plot(traces, output_path="trace_summary_stats.png"):
    """Create plots showing summary statistics across steps."""
    num_samples, num_steps, seq_len = traces.shape
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    steps = list(range(1, num_steps + 1))
    unique_tokens_per_step = []
    total_changes_per_step = []
    mask_token_counts = []
    entropy_per_step = []
    
    mask_token_id = 4096  # Assuming this is the mask token
    
    for step in range(num_steps):
        step_data = traces[:, step, :]  # [num_samples, seq_len]
        
        # Count unique tokens across all samples at this step
        unique_tokens = len(np.unique(step_data))
        unique_tokens_per_step.append(unique_tokens)
        
        # Count mask tokens
        mask_count = np.sum(step_data == mask_token_id)
        mask_token_counts.append(mask_count)
        
        # Count changes from previous step
        if step > 0:
            prev_step_data = traces[:, step-1, :]
            changes = np.sum(step_data != prev_step_data)
            total_changes_per_step.append(changes)
        else:
            total_changes_per_step.append(0)
        
        # Calculate entropy across all positions and samples
        flat_data = step_data.flatten()
        non_mask_data = flat_data[flat_data != mask_token_id]
        if len(non_mask_data) > 0:
            unique_vals, counts = np.unique(non_mask_data, return_counts=True)
            probs = counts / len(non_mask_data)
            entropy = -np.sum(probs * np.log2(probs + 1e-10))
            entropy_per_step.append(entropy)
        else:
            entropy_per_step.append(0)
    
    # Plot 1: Unique tokens per step
    ax1.plot(steps, unique_tokens_per_step, marker='o', color='blue', linewidth=2)
    ax1.set_title('Unique Tokens per Step')
    ax1.set_xlabel('Step')
    ax1.set_ylabel('Unique Tokens')
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Total changes per step
    ax2.bar(steps, total_changes_per_step, color='lightgreen', alpha=0.7)
    ax2.set_title('Token Changes per Step')
    ax2.set_xlabel('Step')
    ax2.set_ylabel('Total Changes')
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Mask token count per step
    ax3.plot(steps, mask_token_counts, marker='s', color='red', linewidth=2)
    ax3.set_title('Mask Token Count per Step')
    ax3.set_xlabel('Step')
    ax3.set_ylabel('Mask Tokens')
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Entropy per step
    ax4.plot(steps, entropy_per_step, marker='^', color='purple', linewidth=2)
    ax4.set_title('Token Entropy per Step')
    ax4.set_xlabel('Step')
    ax4.set_ylabel('Entropy')
    ax4.grid(True, alpha=0.3)
    
    plt.suptitle('Token Generation Statistics Summary', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Summary statistics plot saved to: {output_path}")
    plt.close()


def create_position_evolution_plot(traces, positions_to_track=None, output_path="position_evolution.png"):
    """Create line plots showing how specific positions evolve across steps."""
    num_samples, num_steps, seq_len = traces.shape
    
    if positions_to_track is None:
        # Select some interesting positions to track (first, middle, last, and some random)
        positions_to_track = [0, seq_len//4, seq_len//2, 3*seq_len//4, seq_len-1]
        # Add a few random positions
        np.random.seed(42)
        random_positions = np.random.choice(seq_len, 3, replace=False)
        positions_to_track.extend(random_positions)
        positions_to_track = sorted(list(set(positions_to_track)))[:8]  # Limit to 8 positions
    
    plt.figure(figsize=(15, 8))
    
    # Create color map for different positions
    colors = plt.cm.tab10(np.linspace(0, 1, len(positions_to_track)))
    
    for i, position in enumerate(positions_to_track):
        steps = []
        unique_token_counts = []
        
        for step in range(num_steps):
            steps.append(step + 1)
            # Count unique tokens at this position across all samples
            tokens_at_pos = traces[:, step, position]
            unique_count = len(np.unique(tokens_at_pos))
            unique_token_counts.append(unique_count)
        
        plt.plot(steps, unique_token_counts, marker='o', linewidth=2, markersize=4, 
                label=f'Position {position}', color=colors[i], alpha=0.8)
    
    plt.xlabel('Generation Step', fontsize=12)
    plt.ylabel('Unique Token Count', fontsize=12)
    plt.title('Token Diversity Evolution by Position', fontsize=14, fontweight='bold')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Position evolution plot saved to: {output_path}")
    plt.close()


def plot_heatmap(heatmap_data, title, output_path, cmap='viridis'):
    """Plot and save single heatmap."""
    plt.figure(figsize=(12, 8))
    
    # Create heatmap
    sns.heatmap(
        heatmap_data,
        cmap=cmap,
        cbar_kws={'label': 'Value'},
        xticklabels=False,  # Too many positions to show all labels
        yticklabels=True
    )
    
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel('Token Position', fontsize=12)
    plt.ylabel('Generation Step', fontsize=12)
    
    # Add some x-axis labels at key positions
    seq_len = heatmap_data.shape[1]
    tick_positions = [0, seq_len//4, seq_len//2, 3*seq_len//4, seq_len-1]
    plt.xticks(tick_positions, [str(pos) for pos in tick_positions])
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Saved heatmap to: {output_path}")
    plt.show()


def plot_combined_heatmaps(all_heatmap, condition_heatmaps, condition_labels, title_prefix, output_path, cmap='viridis'):
    """Plot aggregated heatmap with 3 condition-specific heatmaps in a row."""
    fig, axes = plt.subplots(1, 4, figsize=(20, 5))
    
    # Plot aggregated heatmap (use its own scale)
    sns.heatmap(
        all_heatmap,
        ax=axes[0],
        cmap=cmap,
        cbar=True,
        cbar_kws={'label': 'All Classes'},
        xticklabels=False,
        yticklabels=True
    )
    axes[0].set_title(f'{title_prefix}\n(All Classes)', fontsize=12, fontweight='bold')
    axes[0].set_xlabel('Token Position', fontsize=10)
    axes[0].set_ylabel('Generation Step', fontsize=10)
    
    # Plot condition-specific heatmaps (each with its own scale)
    for i, (heatmap, label) in enumerate(zip(condition_heatmaps, condition_labels)):
        ax = axes[i + 1]
        
        # Use individual normalization for each condition
        sns.heatmap(
            heatmap,
            ax=ax,
            cmap=cmap,
            cbar=(i == 2),  # Only show colorbar on the last subplot
            cbar_kws={'label': f'Class {label}'} if i == 2 else None,
            xticklabels=False,
            yticklabels=False
        )
        ax.set_title(f'{title_prefix}\n(Class {label})', fontsize=12, fontweight='bold')
        ax.set_xlabel('Token Position', fontsize=10)
        
        # Add x-axis ticks for better readability
        seq_len = heatmap.shape[1]
        tick_positions = [0, seq_len//2, seq_len-1]
        ax.set_xticks(tick_positions)
        ax.set_xticklabels([str(pos) for pos in tick_positions])
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"Saved combined heatmap to: {output_path}")
    plt.show()


def main():
    parser = argparse.ArgumentParser(description='Plot token frequency heatmap from trace data')
    parser.add_argument('npz_file', type=str, help='Path to NPZ file containing traces')
    parser.add_argument('--output-dir', type=str, default='plots', 
                       help='Output directory for plots (default: plots)')
    parser.add_argument('--mask-token-id', type=int, default=4096,
                       help='Mask token ID to exclude from diversity calculation (default: 4096)')
    parser.add_argument('--plot-type', type=str, choices=['frequency', 'diversity', 'both', 'all'], 
                       default='both', help='Type of heatmap to plot (default: both)')
    parser.add_argument('--enhanced-analysis', action='store_true',
                       help='Generate additional analysis plots (token changes, evolution, summary stats)')
    
    args = parser.parse_args()
    
    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # Load traces and conditions
    traces, conditions = load_traces(args.npz_file)
    
    # Get base filename for output
    npz_path = Path(args.npz_file)
    base_name = npz_path.stem
    
    # Select 3 random conditions if available
    random_conditions = None
    condition_masks = None
    if conditions is not None:
        unique_conditions = np.unique(conditions)
        if len(unique_conditions) >= 3:
            np.random.seed(42)  # For reproducible results
            random_conditions = np.random.choice(unique_conditions, 3, replace=False)
            condition_masks = [conditions == cond for cond in random_conditions]
            print(f"Selected random conditions for detailed analysis: {random_conditions}")
        else:
            print(f"Not enough unique conditions ({len(unique_conditions)}) for detailed analysis")
    
    # Enhanced analysis if requested
    if args.enhanced_analysis or args.plot_type == 'all':
        print("\nGenerating enhanced analysis plots...")
        
        # Analyze token changes
        step_position_changes = analyze_token_changes(traces)
        
        if random_conditions is not None:
            # Compute condition-specific token changes
            condition_token_changes = []
            for mask in condition_masks:
                cond_changes = analyze_token_changes(traces, mask)
                condition_token_changes.append(cond_changes)
            
            # Create combined token change heatmap
            change_combined_output = output_dir / f"{base_name}_token_changes_combined_heatmap.png"
            create_combined_token_change_heatmaps(
                step_position_changes,
                condition_token_changes,
                random_conditions,
                traces.shape[1],
                traces.shape[2],
                change_combined_output
            )
        else:
            # Create single token change heatmap
            change_heatmap_output = output_dir / f"{base_name}_token_changes_heatmap.png"
            create_token_change_heatmap(step_position_changes, traces.shape[1], traces.shape[2], change_heatmap_output)
        
        # Create step distribution charts
        step_dist_output = output_dir / f"{base_name}_step_distributions.png"
        create_step_distribution_charts(step_position_changes, traces.shape[1], step_dist_output)
        
        # Create summary statistics plot
        summary_stats_output = output_dir / f"{base_name}_summary_statistics.png"
        create_summary_statistics_plot(traces, summary_stats_output)
        
        # Create position evolution plot
        evolution_output = output_dir / f"{base_name}_position_evolution.png"
        create_position_evolution_plot(traces, output_path=evolution_output)
        
        # Print token change analysis
        print("\nToken Change Analysis:")
        print("=" * 50)
        total_changes_per_step = []
        for step in range(1, traces.shape[1]):
            if step in step_position_changes:
                step_total = sum(step_position_changes[step].values())
                total_changes_per_step.append(step_total)
                print(f"Step {step + 1}: {step_total} total token changes")
                
                # Show top 5 positions that changed most frequently
                top_changes = step_position_changes[step].most_common(5)
                if top_changes:
                    print(f"  Top changing positions: {', '.join([f'pos{pos}({count})' for pos, count in top_changes])}")
            else:
                total_changes_per_step.append(0)
        
        if total_changes_per_step:
            print(f"\nTotal changes across all steps: {sum(total_changes_per_step)}")
            print(f"Average changes per step: {np.mean(total_changes_per_step):.1f}")
            print(f"Peak changes in step: {np.argmax(total_changes_per_step) + 2} ({max(total_changes_per_step)} changes)")
        
        # Print condition-specific token change statistics
        if random_conditions is not None:
            print(f"\nCondition-Specific Token Change Statistics:")
            print("=" * 50)
            for i, cond in enumerate(random_conditions):
                cond_changes = condition_token_changes[i]
                total_cond_changes = sum(sum(counter.values()) for counter in cond_changes.values())
                print(f"Class {cond}: {total_cond_changes} total changes")
                
                # Find most active step for this condition
                step_totals = []
                for step in range(1, traces.shape[1]):
                    if step in cond_changes:
                        step_total = sum(cond_changes[step].values())
                        step_totals.append(step_total)
                    else:
                        step_totals.append(0)
                
                if step_totals and max(step_totals) > 0:
                    peak_step = np.argmax(step_totals) + 2
                    peak_changes = max(step_totals)
                    avg_changes = np.mean(step_totals)
                    print(f"  Peak activity: Step {peak_step} ({peak_changes} changes)")
                    print(f"  Average per step: {avg_changes:.1f}")
                    
                    # Show top changing position for this condition
                    all_pos_changes = Counter()
                    for step_counter in cond_changes.values():
                        for pos, count in step_counter.items():
                            all_pos_changes[pos] += count
                    
                    if all_pos_changes:
                        top_pos, top_count = all_pos_changes.most_common(1)[0]
                        print(f"  Most active position: {top_pos} ({top_count} total changes)")
        
        # Optimal step-position assignment analysis
        if HAS_SCIPY:
            print("\nOptimal Step-Position Assignment:")
            print("=" * 50)
            opt_assignment, total_score = optimal_step_position_assignment(step_position_changes)
            if opt_assignment:
                print("Optimal generation order (position per step):")
                optimal_order = []
                for step in sorted(opt_assignment.keys(), key=_natural_num):
                    pos = opt_assignment[step]
                    optimal_order.append(pos)
                    print(f"  Step {step + 1}: Position {pos}")
                print(f"\nOptimal order: {optimal_order}")
                print(f"Total assignment score: {total_score}")
        else:
            print("\nNote: Install scipy for optimal step-position assignment analysis")
    
    if args.plot_type in ['frequency', 'both', 'all']:
        # Compute and plot token frequency heatmap
        print("Computing token frequency heatmap...")
        frequency_heatmap = compute_token_frequency_heatmap(traces)
        
        if random_conditions is not None:
            # Compute condition-specific heatmaps
            condition_frequency_heatmaps = []
            for mask in condition_masks:
                cond_heatmap = compute_token_frequency_heatmap(traces, mask)
                condition_frequency_heatmaps.append(cond_heatmap)
            
            # Plot combined heatmaps
            frequency_output = output_dir / f"{base_name}_token_frequency_combined_heatmap.png"
            plot_combined_heatmaps(
                frequency_heatmap,
                condition_frequency_heatmaps,
                random_conditions,
                "Token Frequency",
                frequency_output,
                cmap='Blues'
            )
        else:
            # Plot single aggregated heatmap
            frequency_output = output_dir / f"{base_name}_token_frequency_heatmap.png"
            plot_heatmap(
                frequency_heatmap,
                f"Token Frequency Heatmap\n{base_name}",
                frequency_output,
                cmap='Blues'
            )
    
    if args.plot_type in ['diversity', 'both', 'all']:
        # Compute and plot token diversity heatmap
        print("Computing token diversity heatmap...")
        diversity_heatmap = compute_token_diversity_heatmap(traces, args.mask_token_id)
        
        if random_conditions is not None:
            # Compute condition-specific heatmaps
            condition_diversity_heatmaps = []
            for mask in condition_masks:
                cond_heatmap = compute_token_diversity_heatmap(traces, args.mask_token_id, mask)
                condition_diversity_heatmaps.append(cond_heatmap)
            
            # Plot combined heatmaps
            diversity_output = output_dir / f"{base_name}_token_diversity_combined_heatmap.png"
            plot_combined_heatmaps(
                diversity_heatmap,
                condition_diversity_heatmaps,
                random_conditions,
                "Token Diversity (Entropy)",
                diversity_output,
                cmap='viridis'
            )
        else:
            # Plot single aggregated heatmap
            diversity_output = output_dir / f"{base_name}_token_diversity_heatmap.png"
            plot_heatmap(
                diversity_heatmap,
                f"Token Diversity Heatmap (Entropy)\n{base_name}",
                diversity_output,
                cmap='viridis'
            )
    
    # Print some statistics
    print(f"\nTrace Statistics:")
    print(f"  Number of samples: {traces.shape[0]}")
    print(f"  Number of steps: {traces.shape[1]}")
    print(f"  Sequence length: {traces.shape[2]}")
    print(f"  Token value range: {traces.min()} - {traces.max()}")
    
    if conditions is not None:
        print(f"\nCondition Statistics:")
        print(f"  Number of conditions: {len(conditions)}")
        print(f"  Unique classes: {len(np.unique(conditions))}")
        print(f"  Class range: {conditions.min()} - {conditions.max()}")
    
    if args.plot_type in ['frequency', 'both', 'all']:
        print(f"\nFrequency Heatmap Statistics:")
        print(f"  Min frequency: {frequency_heatmap.min()}")
        print(f"  Max frequency: {frequency_heatmap.max()}")
        print(f"  Mean frequency: {frequency_heatmap.mean():.2f}")
        
        if random_conditions is not None:
            print(f"  Condition-specific statistics:")
            for i, cond in enumerate(random_conditions):
                cond_heatmap = condition_frequency_heatmaps[i]
                print(f"    Class {cond}: min={cond_heatmap.min()}, max={cond_heatmap.max()}, mean={cond_heatmap.mean():.2f}")
    
    if args.plot_type in ['diversity', 'both', 'all']:
        print(f"\nDiversity Heatmap Statistics:")
        print(f"  Min diversity: {diversity_heatmap.min():.3f}")
        print(f"  Max diversity: {diversity_heatmap.max():.3f}")
        print(f"  Mean diversity: {diversity_heatmap.mean():.3f}")
        
        if random_conditions is not None:
            print(f"  Condition-specific statistics:")
            for i, cond in enumerate(random_conditions):
                cond_heatmap = condition_diversity_heatmaps[i]
                print(f"    Class {cond}: min={cond_heatmap.min():.3f}, max={cond_heatmap.max():.3f}, mean={cond_heatmap.mean():.3f}")


if __name__ == "__main__":
    main()
